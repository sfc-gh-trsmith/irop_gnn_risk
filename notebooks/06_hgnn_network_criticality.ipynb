{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68686101",
   "metadata": {},
   "source": [
    "# Heterogeneous Graph Neural Network - Network Criticality\n",
    "\n",
    "Implements a Heterogeneous GNN using PyTorch Geometric on Snowpark Container Services.\n",
    "This notebook builds a dynamic graph with flights, aircraft, crew, passengers, and airports\n",
    "as nodes, then uses attention-based message passing to compute network criticality scores.\n",
    "\n",
    "**Target:** `network_disruption_label` (downstream cascade impact)\n",
    "**Algorithm:** HGTConv (Heterogeneous Graph Transformer)\n",
    "**Output:** `IROP_GNN_RISK.ML_PROCESSING.GNN_FLIGHT_EMBEDDINGS`\n",
    "\n",
    "**Runtime:** SPCS GPU Compute Pool (IROP_GNN_RISK_GPU_POOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15291237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.nn import HGTConv, Linear\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark.functions import col\n",
    "from snowflake.ml.registry import Registry\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2487431",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = get_active_session()\n",
    "session.use_database('IROP_GNN_RISK')\n",
    "session.use_schema('ATOMIC')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Connected: {session.get_current_database()}.{session.get_current_schema()}\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e4cc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_pd = session.table('FLIGHT_INSTANCE').to_pandas()\n",
    "rotations_pd = session.table('AIRCRAFT_ROTATION').to_pandas()\n",
    "crew_pd = session.table('CREW_DUTY_PERIOD').to_pandas()\n",
    "assignments_pd = session.table('CREW_ASSIGNMENT').to_pandas()\n",
    "airports_pd = session.table('AIRPORT_CAPABILITY').to_pandas()\n",
    "\n",
    "print(f\"Loaded: {len(flights_pd)} flights, {len(rotations_pd)} rotations\")\n",
    "print(f\"        {len(crew_pd)} crew duties, {len(assignments_pd)} assignments\")\n",
    "print(f\"        {len(airports_pd)} airports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489c11d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_idx = {fk: i for i, fk in enumerate(flights_pd['FLIGHT_KEY'].unique())}\n",
    "tail_idx = {tn: i for i, tn in enumerate(rotations_pd['TAIL_NUMBER'].unique())}\n",
    "duty_idx = {di: i for i, di in enumerate(crew_pd['DUTY_ID'].unique())}\n",
    "airport_idx = {ac: i for i, ac in enumerate(airports_pd['STATION_CODE'].unique())}\n",
    "\n",
    "print(f\"Node counts: Flights={len(flight_idx)}, Tails={len(tail_idx)}\")\n",
    "print(f\"             Duties={len(duty_idx)}, Airports={len(airport_idx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29140835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hetero_graph():\n",
    "    data = HeteroData()\n",
    "    \n",
    "    flight_features = flights_pd[['CURRENT_DELAY_DEPARTURE', 'TURN_BUFFER_MINUTES', \n",
    "                                  'PAX_COUNT', 'CONNECTING_PAX_PCT', 'REVENUE_AT_RISK_USD',\n",
    "                                  'DELAY_RISK_SCORE', 'TURN_SUCCESS_PROB', 'MISCONNECT_PROB']].fillna(0).values\n",
    "    data['flight'].x = torch.tensor(flight_features, dtype=torch.float)\n",
    "    \n",
    "    tail_features = []\n",
    "    for tail in tail_idx.keys():\n",
    "        tail_data = rotations_pd[rotations_pd['TAIL_NUMBER'] == tail].iloc[0] if len(rotations_pd[rotations_pd['TAIL_NUMBER'] == tail]) > 0 else None\n",
    "        if tail_data is not None:\n",
    "            tail_features.append([\n",
    "                float(tail_data.get('AIRCRAFT_AGE_YEARS', 0) or 0),\n",
    "                float(tail_data.get('UTILIZATION_HOURS_24H', 0) or 0),\n",
    "                float(1 if tail_data.get('MEL_APU_FLAG', False) else 0),\n",
    "                float(tail_data.get('AOG_RISK_SCORE', 0) or 0)\n",
    "            ])\n",
    "        else:\n",
    "            tail_features.append([0, 0, 0, 0])\n",
    "    data['aircraft'].x = torch.tensor(tail_features, dtype=torch.float)\n",
    "    \n",
    "    duty_features = crew_pd[['FDP_LIMIT_MINUTES', 'FDP_TIME_USED_MINUTES', \n",
    "                             'FDP_REMAINING_MINUTES', 'NUM_SEGMENTS',\n",
    "                             'CREW_TIMEOUT_RISK_SCORE']].fillna(0).values\n",
    "    data['crew'].x = torch.tensor(duty_features, dtype=torch.float)\n",
    "    \n",
    "    airport_features = airports_pd[['GATE_COUNT', 'ATC_CONGESTION_INDEX',\n",
    "                                    'AIRPORT_DISRUPTION_INDEX']].fillna(0).values\n",
    "    data['airport'].x = torch.tensor(airport_features, dtype=torch.float)\n",
    "    \n",
    "    operated_by_src, operated_by_dst = [], []\n",
    "    for _, row in rotations_pd.iterrows():\n",
    "        if row['FLIGHT_KEY'] in flight_idx and row['TAIL_NUMBER'] in tail_idx:\n",
    "            operated_by_src.append(flight_idx[row['FLIGHT_KEY']])\n",
    "            operated_by_dst.append(tail_idx[row['TAIL_NUMBER']])\n",
    "    data['flight', 'operated_by', 'aircraft'].edge_index = torch.tensor([operated_by_src, operated_by_dst], dtype=torch.long)\n",
    "    \n",
    "    next_leg_src, next_leg_dst = [], []\n",
    "    for _, row in rotations_pd.iterrows():\n",
    "        if row['FLIGHT_KEY'] in flight_idx and row['NEXT_FLIGHT_KEY'] in flight_idx:\n",
    "            next_leg_src.append(flight_idx[row['FLIGHT_KEY']])\n",
    "            next_leg_dst.append(flight_idx[row['NEXT_FLIGHT_KEY']])\n",
    "    data['flight', 'next_leg', 'flight'].edge_index = torch.tensor([next_leg_src, next_leg_dst], dtype=torch.long)\n",
    "    \n",
    "    assigned_src, assigned_dst = [], []\n",
    "    for _, row in assignments_pd.iterrows():\n",
    "        if row['FLIGHT_KEY'] in flight_idx and row['DUTY_ID'] in duty_idx:\n",
    "            assigned_src.append(flight_idx[row['FLIGHT_KEY']])\n",
    "            assigned_dst.append(duty_idx[row['DUTY_ID']])\n",
    "    data['flight', 'assigned_to', 'crew'].edge_index = torch.tensor([assigned_src, assigned_dst], dtype=torch.long)\n",
    "    \n",
    "    departs_src, departs_dst, arrives_src, arrives_dst = [], [], [], []\n",
    "    for _, row in flights_pd.iterrows():\n",
    "        if row['FLIGHT_KEY'] in flight_idx:\n",
    "            if row['DEPARTURE_STATION'] in airport_idx:\n",
    "                departs_src.append(flight_idx[row['FLIGHT_KEY']])\n",
    "                departs_dst.append(airport_idx[row['DEPARTURE_STATION']])\n",
    "            if row['ARRIVAL_STATION'] in airport_idx:\n",
    "                arrives_src.append(flight_idx[row['FLIGHT_KEY']])\n",
    "                arrives_dst.append(airport_idx[row['ARRIVAL_STATION']])\n",
    "    data['flight', 'departs_from', 'airport'].edge_index = torch.tensor([departs_src, departs_dst], dtype=torch.long)\n",
    "    data['flight', 'arrives_at', 'airport'].edge_index = torch.tensor([arrives_src, arrives_dst], dtype=torch.long)\n",
    "    \n",
    "    return data\n",
    "\n",
    "hetero_data = build_hetero_graph()\n",
    "print(f\"Graph built: {hetero_data}\")\n",
    "print(f\"Node types: {hetero_data.node_types}\")\n",
    "print(f\"Edge types: {hetero_data.edge_types}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d68366e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HGNNNetworkCriticality(nn.Module):\n",
    "    def __init__(self, hidden_dim=64, num_heads=4, num_layers=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.node_encoders = nn.ModuleDict({\n",
    "            'flight': Linear(8, hidden_dim),\n",
    "            'aircraft': Linear(4, hidden_dim),\n",
    "            'crew': Linear(5, hidden_dim),\n",
    "            'airport': Linear(3, hidden_dim)\n",
    "        })\n",
    "        \n",
    "        self.convs = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            conv = HGTConv(\n",
    "                in_channels=hidden_dim,\n",
    "                out_channels=hidden_dim,\n",
    "                metadata=(\n",
    "                    ['flight', 'aircraft', 'crew', 'airport'],\n",
    "                    [('flight', 'operated_by', 'aircraft'),\n",
    "                     ('flight', 'next_leg', 'flight'),\n",
    "                     ('flight', 'assigned_to', 'crew'),\n",
    "                     ('flight', 'departs_from', 'airport'),\n",
    "                     ('flight', 'arrives_at', 'airport')]\n",
    "                ),\n",
    "                heads=num_heads\n",
    "            )\n",
    "            self.convs.append(conv)\n",
    "        \n",
    "        self.criticality_head = nn.Sequential(\n",
    "            Linear(hidden_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        h_dict = {ntype: encoder(x_dict[ntype]) for ntype, encoder in self.node_encoders.items()}\n",
    "        \n",
    "        for conv in self.convs:\n",
    "            h_dict = conv(h_dict, edge_index_dict)\n",
    "            h_dict = {k: F.relu(v) for k, v in h_dict.items()}\n",
    "        \n",
    "        criticality = self.criticality_head(h_dict['flight']) * 100\n",
    "        \n",
    "        return h_dict, criticality\n",
    "\n",
    "model = HGNNNetworkCriticality(hidden_dim=64, num_heads=4, num_layers=2).to(device)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e70ccf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_criticality = torch.tensor(\n",
    "    flights_pd['NETWORK_CRITICALITY_SCORE'].fillna(50).values,\n",
    "    dtype=torch.float\n",
    ").unsqueeze(1).to(device)\n",
    "\n",
    "hetero_data = hetero_data.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    h_dict, pred_criticality = model(hetero_data.x_dict, hetero_data.edge_index_dict)\n",
    "    loss = criterion(pred_criticality, target_criticality)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1}: Loss = {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e760402",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    embeddings_dict, criticality_scores = model(hetero_data.x_dict, hetero_data.edge_index_dict)\n",
    "    \n",
    "flight_embeddings = embeddings_dict['flight'].cpu().numpy()\n",
    "criticality_scores = criticality_scores.cpu().numpy().flatten()\n",
    "\n",
    "print(f\"Flight embeddings shape: {flight_embeddings.shape}\")\n",
    "print(f\"Criticality scores range: [{criticality_scores.min():.1f}, {criticality_scores.max():.1f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88553cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_keys = list(flight_idx.keys())\n",
    "tail_numbers = [rotations_pd[rotations_pd['FLIGHT_KEY'] == fk]['TAIL_NUMBER'].iloc[0] \n",
    "                if len(rotations_pd[rotations_pd['FLIGHT_KEY'] == fk]) > 0 else None \n",
    "                for fk in flight_keys]\n",
    "\n",
    "next_leg_counts = rotations_pd.groupby('FLIGHT_KEY')['NEXT_FLIGHT_KEY'].apply(\n",
    "    lambda x: x.notna().sum()\n",
    ").to_dict()\n",
    "\n",
    "output_data = []\n",
    "for i, fk in enumerate(flight_keys):\n",
    "    output_data.append({\n",
    "        'EMBEDDING_ID': str(uuid.uuid4())[:8].upper(),\n",
    "        'FLIGHT_KEY': fk,\n",
    "        'TAIL_NUMBER': tail_numbers[i],\n",
    "        'SNAPSHOT_TS': pd.Timestamp.now(),\n",
    "        'GNN_EMBEDDING': flight_embeddings[i].tolist(),\n",
    "        'GNN_NETWORK_CRITICALITY': float(criticality_scores[i]),\n",
    "        'ATTENTION_WEIGHTS': None,\n",
    "        'DOWNLINE_LEGS_AFFECTED_COUNT': next_leg_counts.get(fk, 0),\n",
    "        'MODEL_VERSION': 'v1.0'\n",
    "    })\n",
    "\n",
    "output_df = session.create_dataframe(output_data)\n",
    "session.use_schema('ML_PROCESSING')\n",
    "output_df.write.mode('overwrite').save_as_table('GNN_FLIGHT_EMBEDDINGS')\n",
    "print(f\"Saved {len(output_data)} GNN embeddings to ML_PROCESSING.GNN_FLIGHT_EMBEDDINGS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cdb454",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '/tmp/hgnn_model.pt')\n",
    "\n",
    "print(\"Model saved. To register in Snowflake ML Registry:\")\n",
    "print(\"  1. Upload model artifacts to stage\")\n",
    "print(\"  2. Use Registry.log_model() with custom model class\")\n",
    "print(\"  3. Deploy for inference via Model Registry\")\n",
    "\n",
    "print(f\"\\nTop 10 flights by network criticality:\")\n",
    "top_flights = sorted(zip(flight_keys, criticality_scores), key=lambda x: x[1], reverse=True)[:10]\n",
    "for fk, score in top_flights:\n",
    "    print(f\"  {fk}: {score:.1f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
